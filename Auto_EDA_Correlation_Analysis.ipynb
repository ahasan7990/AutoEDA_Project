{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.0 About Author:-\n",
    "\n",
    "  - 1.1 Description : Complete Auto EDA Analysis with multiple Excel sheet\n",
    "\n",
    "  - 1.2 Date of Submission: 25 July 2025\n",
    "\n",
    "  - 1.3 Author Name: Ahasan U Haque\n",
    "\n",
    "  - 1.4 Email: ahasan7990@gmail.com\n",
    "\n",
    "  - 1.5 kaggle_id: ansarianam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.0 About Dataset\n",
    "\n",
    "  - 2.1- Description: This dataset is downloaded from Keggal , link https://www.kaggle.com/datasets/vivekkumarkamat/vendor-performance-analysis-project\n",
    "  \n",
    "\n",
    "  - 2.2- Context :\n",
    "\n",
    "While many public datasets (on Kaggle and the like) provide for EDA Store data, there are not many counterpart datasets available for Store anywhere on the web.\n",
    "\n",
    "  - 2.3- Content:\n",
    "Each Table (row) has values for Store salesprice, Inovicedate and many more in multiple sheets.\n",
    "\n",
    "  - 2.4- Acknowledgements:\n",
    "This information is scraped from the Kaggle dataset. This Store information would not be available without it.\n",
    "\n",
    "  - 2.5- Inspiration: Auto EDA and Correlation Analysis Notebook\n",
    "    - This notebook performs the following tasks:\n",
    "    - Loads and cleans multiple CSV files\n",
    "    - Performs exploratory data analysis (EDA)\n",
    "    - Detects and handles missing values and outliers\n",
    "    - Correlates across tables\n",
    "    - Exports results to Excel and Word reports\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3.0-Task:-\n",
    "\n",
    "    - 3.1- We have to do Explorartory Data Analysis(EDA) on Store dataset. The tasks involved in this EDA include data cleaning to handle missing or inconsistent data, data visualization to identify patterns and outliers and statistical analysis to understand relationships between variables. We will also focus on feature engineering to extract meaningful information for further analysis.\n",
    "\n",
    "   - 3.2- Objective:- The primary objective of this Exploratory Data Analysis (EDA) on the  Store dataset is to understand the underlying patterns, trends, and relationships between different variables. This will help us gain insights into revenue gain for the store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.0- Importing Liberaries :\n",
    "\n",
    "    We will useing following libraries to complete our Analysis.\n",
    "\n",
    "    - Pandas: Provides data structures and functions needed for manipulating structured data.\n",
    "    - NumPy: Used for numerical computing, such as arrays and linear algebra.\n",
    "    - Matplotlib: Allows creation of static, animated, and interactive visualizations in Python.\n",
    "    - Seaborn: Based on Matplotlib, it provides a high-level interface for drawing attractive    statistical graphics.\n",
    "    - SciPy: Used for scientific computing and technical computing.\n",
    "    - sklearn: Provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. It’s used for data mining, data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AHASANUL HAQUE\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 1: Configuration and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import openpyxl\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Define Paths and Parameters\n",
    "DATA_DIR = r\"D:/Auto EDR/Data\"\n",
    "EDA_OUTPUT_DIR = \"eda_outputs\"\n",
    "MASTER_CSV_PATH = os.path.join(EDA_OUTPUT_DIR, \"master_data.csv\")\n",
    "MERGE_LIMIT = 800\n",
    "\n",
    "os.makedirs(EDA_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded D:/Auto EDR/Data\\Sales.csv (5000 rows)\n",
      "Loaded D:/Auto EDR/Data\\Purchases.csv (5000 rows)\n",
      "Loaded D:/Auto EDR/Data\\vendor_invoice.csv (5000 rows)\n",
      "Loaded D:/Auto EDR/Data\\purchase_prices.csv (5000 rows)\n",
      "Loaded D:/Auto EDR/Data\\begin_inventory.csv (5000 rows)\n",
      "Loaded D:/Auto EDR/Data\\end_inventory.csv (5000 rows)\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Read Limited Rows from Each CSV\n",
    "def read_csv_limited(filepath, nrows=5000):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, nrows=nrows, encoding='utf-8', delimiter=',', on_bad_lines='skip')\n",
    "        print(f\"Loaded {filepath} ({len(df)} rows)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading {filepath}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "files = {\n",
    "    \"sales\": \"Sales.csv\",\n",
    "    \"purchases\": \"Purchases.csv\",\n",
    "    \"vendor_invoice\": \"vendor_invoice.csv\",\n",
    "    \"purchase_prices\": \"purchase_prices.csv\",\n",
    "    \"begin_inventory\": \"begin_inventory.csv\",\n",
    "    \"end_inventory\": \"end_inventory.csv\",\n",
    "}\n",
    "\n",
    "dataframes = {name: read_csv_limited(os.path.join(DATA_DIR, fname)) for name, fname in files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Part 4: Clean DataFrame Columns and Drop All-Null Rows\n",
    "def clean_data(df):\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    return df\n",
    "\n",
    "dataframes = {name: clean_data(df) for name, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Part 5: Detect and Handle Outliers (IQR Method)\n",
    "def remove_outliers_iqr(df):\n",
    "    df_clean = df.copy()\n",
    "    numeric_cols = df_clean.select_dtypes(include=np.number)\n",
    "    Q1 = numeric_cols.quantile(0.25)\n",
    "    Q3 = numeric_cols.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    is_outlier = ((numeric_cols < (Q1 - 1.5 * IQR)) | (numeric_cols > (Q3 + 1.5 * IQR)))\n",
    "    df_clean[is_outlier] = np.nan\n",
    "    return df_clean\n",
    "\n",
    "dataframes = {name: remove_outliers_iqr(df) for name, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nulls handled in 'sales'\n",
      "✅ Nulls handled in 'purchases'\n",
      "✅ Nulls handled in 'vendor_invoice'\n",
      "✅ Nulls handled in 'purchase_prices'\n",
      "✅ Nulls handled in 'begin_inventory'\n",
      "✅ Nulls handled in 'end_inventory'\n"
     ]
    }
   ],
   "source": [
    "# Part 5.1: Clean and Handle Null Values Across All DataFrames\n",
    "\n",
    "def handle_nulls(df, strategy=\"auto\"):\n",
    "    \"\"\"\n",
    "    Handles missing values in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        strategy (str): Strategy for imputation: \"auto\", \"mean\", \"median\", \"mode\", or \"drop\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with handled nulls.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() == 0:\n",
    "            continue  # No nulls in this column\n",
    "        \n",
    "        if strategy == \"drop\":\n",
    "            df = df.dropna(subset=[col])\n",
    "        \n",
    "        elif strategy == \"mean\" and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        \n",
    "        elif strategy == \"median\" and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        elif strategy == \"mode\":\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        \n",
    "        elif strategy == \"auto\":\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "        \n",
    "        else:\n",
    "            df[col] = df[col].fillna(\"Unknown\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply null handling to all dataframes\n",
    "dataframes_cleaned = {}\n",
    "for name, df in dataframes.items():\n",
    "    cleaned_df = handle_nulls(df, strategy=\"auto\")\n",
    "    dataframes_cleaned[name] = cleaned_df\n",
    "    print(f\"✅ Nulls handled in '{name}'\")\n",
    "\n",
    "# Update the dataframes reference to cleaned version\n",
    "dataframes = dataframes_cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Master CSV written to: eda_outputs\\master_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 6: Merge All CSVs into a Master File (max 5000 rows)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "MERGE_LIMIT = 5000\n",
    "OUTPUT_DIR = \"eda_outputs\"\n",
    "MASTER_CSV_PATH = os.path.join(OUTPUT_DIR, \"master_data.csv\")\n",
    "\n",
    "def create_master_df(dataframes):\n",
    "    common_keys = [\"product_id\", \"item_id\", \"vendor_id\"]\n",
    "    keys_present = [key for key in common_keys if all(key in df.columns for df in dataframes.values())]\n",
    "    \n",
    "    dfs = list(dataframes.values())\n",
    "    if not dfs:\n",
    "        raise ValueError(\"No dataframes to merge.\")\n",
    "    \n",
    "    master_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        for key in keys_present:\n",
    "            if key in df.columns and key in master_df.columns:\n",
    "                master_df = pd.merge(master_df, df, on=key, how='outer')\n",
    "                break  # merge once on first matching key\n",
    "    \n",
    "    return master_df.head(MERGE_LIMIT)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Try writing the master CSV file\n",
    "try:\n",
    "    master_df = create_master_df(dataframes)\n",
    "    master_df.to_csv(MASTER_CSV_PATH, index=False)\n",
    "    print(f\"✅ Master CSV written to: {MASTER_CSV_PATH}\")\n",
    "except PermissionError as e:\n",
    "    print(\"❌ Unable to write the file. Please close the file if it's open elsewhere.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Vendor Performance Analysis (Integrated into EDA Pipeline)\n",
    "\n",
    "# Set a higher row limit for master merge\n",
    "MERGE_LIMIT = 5000  # Increase from default 800 to 5000\n",
    "\n",
    "def vendor_performance_analysis(master_df, output_path=\"eda_outputs/vendor_performance.xlsx\"):\n",
    "    \"\"\"\n",
    "    Analyze vendor performance based on combined sales and purchase data.\n",
    "\n",
    "    Parameters:\n",
    "    - master_df: DataFrame containing merged data from all input tables.\n",
    "    - output_path: Path to save the vendor performance summary Excel file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for required column\n",
    "    if \"vendor_id\" not in master_df.columns:\n",
    "        print(\"❌ 'vendor_id' column not found. Skipping vendor analysis.\")\n",
    "        return\n",
    "\n",
    "    # Create a working copy of the merged dataframe\n",
    "    performance = master_df.copy()\n",
    "\n",
    "    # Convert sales and purchase price columns to numeric (handling errors as NaN)\n",
    "    for col in [\"sales_price\", \"purchase_price\"]:\n",
    "        if col in performance.columns:\n",
    "            performance[col] = pd.to_numeric(performance[col], errors='coerce')\n",
    "\n",
    "    # Group data by vendor_id to calculate performance metrics\n",
    "    vendor_stats = performance.groupby(\"vendor_id\").agg(\n",
    "        total_sales=pd.NamedAgg(column=\"sales_price\", aggfunc=\"sum\"),             # Total sales amount\n",
    "        total_purchases=pd.NamedAgg(column=\"purchase_price\", aggfunc=\"sum\"),      # Total purchases amount\n",
    "        avg_sales_price=pd.NamedAgg(column=\"sales_price\", aggfunc=\"mean\"),        # Average sales price\n",
    "        avg_purchase_price=pd.NamedAgg(column=\"purchase_price\", aggfunc=\"mean\"),  # Average purchase price\n",
    "        transaction_count=pd.NamedAgg(column=\"vendor_id\", aggfunc=\"count\")        # Total number of records per vendor\n",
    "    ).fillna(0)  # Replace NaN with 0 for clean reporting\n",
    "\n",
    "    # Calculate gross margin (sales - purchases)\n",
    "    vendor_stats[\"gross_margin\"] = vendor_stats[\"total_sales\"] - vendor_stats[\"total_purchases\"]\n",
    "\n",
    "    # Save the output to an Excel file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    vendor_stats.to_excel(output_path)\n",
    "\n",
    "    print(f\"✅ Vendor performance report saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 82.43it/s]0<00:00, 28.42it/s, Describe variable: vendorname]\n",
      "Summarize dataset: 100%|██████████| 48/48 [00:04<00:00, 10.95it/s, Completed]                         \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 81.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\sales_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 28.46it/s]0<00:00, 22.07it/s, Describe variable: classification]\n",
      "Summarize dataset: 100%|██████████| 74/74 [00:08<00:00,  8.35it/s, Completed]                           \n",
      "Generate report structure: 100%|██████████| 1/1 [00:04<00:00,  5.00s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 57.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\purchases_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 36.82it/s]0<00:00, 25.20it/s, Describe variable: approval]\n",
      "Summarize dataset: 100%|██████████| 44/44 [00:04<00:00, 10.87it/s, Completed]                         \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.92s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 82.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\vendor_invoice_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 61.43it/s]00<00:00, 25.72it/s, Describe variable: vendorname]  \n",
      "Summarize dataset: 100%|██████████| 34/34 [00:02<00:00, 12.58it/s, Completed]                           \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 94.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\purchase_prices_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 47.03it/s]00<00:00,  8.60it/s, Describe variable: startdate]\n",
      "Summarize dataset: 100%|██████████| 27/27 [00:02<00:00, 13.49it/s, Completed]                  \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 154.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\begin_inventory_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 78.17it/s]00<00:00, 16.78it/s, Describe variable: enddate]\n",
      "Summarize dataset: 100%|██████████| 27/27 [00:01<00:00, 14.04it/s, Completed]                 \n",
      "Generate report structure: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 129.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA report saved: eda_outputs\\end_inventory_EDA.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 8: Generate EDA Reports for Each Table\n",
    "def generate_eda_reports(dataframes):\n",
    "    for name, df in dataframes.items():\n",
    "        report = ProfileReport(df, title=f\"EDA Report: {name}\", explorative=True)\n",
    "        report_path = os.path.join(EDA_OUTPUT_DIR, f\"{name}_EDA.html\")\n",
    "        report.to_file(report_path)\n",
    "        print(f\"EDA report saved: {report_path}\")\n",
    "\n",
    "generate_eda_reports(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Part 8: Inter-Table Correlation Matrix\n",
    "def find_inter_table_correlations(dataframes, output_excel_path):\n",
    "    numeric_frames = [df.select_dtypes(include=np.number).add_prefix(f\"{name}_\") for name, df in dataframes.items()]\n",
    "    merged = pd.concat(numeric_frames, axis=1).dropna(axis=1, how='all')\n",
    "    correlation_matrix = merged.corr()\n",
    "    correlation_matrix.to_excel(output_excel_path, sheet_name='Correlations')\n",
    "    print(f\"Correlation matrix saved to {output_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Part 9: Summary Observations per Table\n",
    "def generate_observation_report(dataframes, output_excel_path):\n",
    "    observations = []\n",
    "    for name, df in dataframes.items():\n",
    "        obs = {\"Table\": name}\n",
    "        num_cols = df.select_dtypes(include='number')\n",
    "        cat_cols = df.select_dtypes(include='object')\n",
    "\n",
    "        obs[\"Rows\"] = df.shape[0]\n",
    "        obs[\"Columns\"] = df.shape[1]\n",
    "        obs[\"Missing %\"] = round(df.isnull().sum().sum() / (df.shape[0]*df.shape[1]) * 100, 2)\n",
    "\n",
    "        if df.isnull().sum().max() > 0:\n",
    "            col = df.isnull().sum().idxmax()\n",
    "            obs[\"Most Missing Column\"] = f\"{col} ({df[col].isnull().sum()} missing)\"\n",
    "        else:\n",
    "            obs[\"Most Missing Column\"] = \"None\"\n",
    "\n",
    "        if not cat_cols.empty:\n",
    "            top_col = cat_cols.nunique().idxmin()\n",
    "            top_val = df[top_col].mode().iloc[0]\n",
    "            freq = df[top_col].value_counts().iloc[0]\n",
    "            obs[\"Most Frequent Value\"] = f\"{top_col}: '{top_val}' ({freq})\"\n",
    "        else:\n",
    "            obs[\"Most Frequent Value\"] = \"N/A\"\n",
    "\n",
    "        if not num_cols.empty:\n",
    "            corr = num_cols.corr().abs()\n",
    "            corr.values[[range(len(corr))]*2] = 0\n",
    "            pair = corr.stack().idxmax()\n",
    "            obs[\"Top Correlation\"] = f\"{pair[0]} & {pair[1]}: {round(corr.max().max(), 2)}\"\n",
    "        else:\n",
    "            obs[\"Top Correlation\"] = \"N/A\"\n",
    "\n",
    "        observations.append(obs)\n",
    "\n",
    "    obs_df = pd.DataFrame(observations)\n",
    "    with pd.ExcelWriter(output_excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        obs_df.to_excel(writer, sheet_name='Observations', index=False)\n",
    "    print(f\"Observation sheet saved in {output_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Part 10: Export to Word Document\n",
    "def export_to_word(dataframes, output_path=\"summary_report.docx\"):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Auto EDA Summary Report\", 0)\n",
    "\n",
    "    for name, df in dataframes.items():\n",
    "        doc.add_heading(f\"{name.capitalize()} Table\", level=1)\n",
    "        doc.add_paragraph(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "        missing = df.isnull().sum()\n",
    "        if missing.sum() > 0:\n",
    "            most_missing = missing.idxmax()\n",
    "            doc.add_paragraph(f\"Most Missing Column: {most_missing} ({missing[most_missing]} values)\")\n",
    "\n",
    "        if not df.select_dtypes(include='object').empty:\n",
    "            col = df.select_dtypes(include='object').nunique().idxmin()\n",
    "            val = df[col].mode().iloc[0]\n",
    "            freq = df[col].value_counts().iloc[0]\n",
    "            doc.add_paragraph(f\"Most Frequent Value: {col} = {val} ({freq}x)\")\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Word report saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix saved to eda_outputs\\inter_table_correlations.xlsx\n",
      "Observation sheet saved in eda_outputs\\inter_table_correlations.xlsx\n",
      "Word report saved to summary_report.docx\n"
     ]
    }
   ],
   "source": [
    "#  Part 11: Run Full Pipeline\n",
    "def full_pipeline():\n",
    "    correlation_output = os.path.join(EDA_OUTPUT_DIR, \"inter_table_correlations.xlsx\")\n",
    "    find_inter_table_correlations(dataframes, correlation_output)\n",
    "    generate_observation_report(dataframes, correlation_output)\n",
    "    export_to_word(dataframes)\n",
    "\n",
    "full_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
